# üü¢ AI OpenAI - Int√©gration OpenAI avec Support O3

## Vue d'Ensemble

L'app `ai_openai` fournit une **int√©gration sp√©cialis√©e** avec l'API OpenAI, √©tendant les jobs IA centraux avec des fonctionnalit√©s avanc√©es : chat completions O3/GPT-4.1, assistants, gestion intelligente des mod√®les et configurations adapt√©es par g√©n√©ration.

### Responsabilit√©
- **OpenAIJob** : Extension des AIJob avec param√®tres OpenAI multi-g√©n√©ration
- **Chat Completions** : Interface unifi√©e pour O3, GPT-4.1 et legacy
- **Auto-Configuration** : D√©tection automatique des param√®tres selon mod√®le
- **Cost Optimization** : Calcul pr√©cis des co√ªts par mod√®le
- **Assistant Integration** : Gestion des assistants OpenAI

### Base URL
```
https://backoffice.humari.fr/ai/openai/
```

---

## üÜï Support Mod√®les Nouvelle G√©n√©ration

### Mod√®les Support√©s

#### **O3 Series (Reasoning Models)**
- `o3` : Mod√®le de raisonnement avanc√©
- `o3-mini` : Version optimis√©e co√ªt/performance

**Sp√©cificit√©s O3** :
- ‚ùå **Pas de temperature** (param√®tre non support√©)
- ‚úÖ **reasoning_effort** : `low`, `medium`, `high`
- ‚úÖ **max_completion_tokens** au lieu de `max_tokens`
- ‚úÖ **Messages structur√©s** automatiques
- ‚úÖ **Role mapping** : `system` ‚Üí `developer`

#### **GPT-4.1 (Enhanced)**
- `gpt-4.1` : Version am√©lior√©e de GPT-4

**Sp√©cificit√©s GPT-4.1** :
- ‚úÖ **temperature** : D√©faut 1.0 (plus cr√©atif)
- ‚úÖ **max_completion_tokens** au lieu de `max_tokens`
- ‚úÖ **Messages structur√©s** automatiques

#### **Legacy Models**
- `gpt-4o`, `gpt-4-turbo`, `gpt-3.5-turbo`

**Sp√©cificit√©s Legacy** :
- ‚úÖ **temperature** : D√©faut 0.7
- ‚úÖ **max_tokens** classique
- ‚úÖ **Messages simples** (cha√Ænes de caract√®res)

---

## üìä Mod√®les de Donn√©es Actualis√©s

### OpenAIJob (Extended)
```python
# Extension sp√©cialis√©e pour AIJob
- ai_job: OneToOne ‚Üí AIJob (relation centrale)

# Config multi-g√©n√©ration
- model: str # "gpt-4o", "o3", "gpt-4.1"
- temperature: float (NULL pour O3) # 0.0-2.0 ou NULL
- max_tokens: int # Legacy seulement
- max_completion_tokens: int # üÜï O3/GPT-4.1 seulement

# üÜï PARAM√àTRES O3 SP√âCIFIQUES
- reasoning_effort: str # 'low', 'medium', 'high' pour O3
- messages_format: str # 'legacy', 'structured'

# Tools et response (inchang√©)
- tools: JSON # Outils disponibles
- tool_resources: JSON # Resources pour tools
- response_format: JSON # Format de r√©ponse

# Response OpenAI (inchang√©)
- openai_id: str # ID completion OpenAI
- completion_tokens: int # Tokens de r√©ponse
- prompt_tokens: int # Tokens de prompt
- total_tokens: int # Total tokens
```

### Nouvelles Propri√©t√©s
```python
@property
def is_o3_model(self):
    """D√©tecte si c'est un mod√®le O3"""
    return self.model.startswith('o3')

@property  
def is_new_generation_model(self):
    """D√©tecte mod√®les nouvelle g√©n√©ration"""
    return self.model.startswith('o3') or self.model in ['gpt-4.1']
```

---

## üéØ Endpoints Actualis√©s

### Chat Completions Multi-Mod√®les

#### `POST /ai/openai/chat/`
**Cr√©ation de chat completion avec auto-configuration**

**Param√®tres Universels** :
```json
{
  "messages": [
    {"role": "user", "content": "Analyse ce contenu SEO..."}
  ],
  "model": "o3",                      // üÜï Support O3
  "description": "Analyse SEO avec O3",
  "tools": [                          // Optionnel, tous mod√®les
    {"type": "file_search"}
  ]
}
```

**Param√®tres Sp√©cifiques par Mod√®le** :

#### **Pour O3 Models** :
```json
{
  "model": "o3",
  "reasoning_effort": "high",         // üÜï REQUIS pour O3
  "max_completion_tokens": 2000,     // üÜï Au lieu de max_tokens
  "response_format": {"type": "text"}
  // ‚ùå temperature non support√©e
}
```

#### **Pour GPT-4.1** :
```json
{
  "model": "gpt-4.1", 
  "temperature": 1.0,                 // D√©faut plus cr√©atif
  "max_completion_tokens": 10000,     // üÜï Nouvelle limite
  "top_p": 1,
  "frequency_penalty": 0,
  "presence_penalty": 0
}
```

#### **Pour Legacy (GPT-4o, etc.)** :
```json
{
  "model": "gpt-4o",
  "temperature": 0.7,                 // D√©faut √©quilibr√©
  "max_tokens": 1000,                 // Format classique
  "tools": [...]
}
```

**Auto-Configuration Intelligente** :
- **D√©tection automatique** du type de mod√®le
- **Param√®tres par d√©faut** optimis√©s selon le mod√®le
- **Conversion automatique** des messages pour nouveaux mod√®les
- **Validation** des param√®tres incompatibles

**R√©ponse Synchrone** :
```json
{
  "job_id": "ai_job_abc123",
  "status": "completed",
  "result": {
    "completion": "Analyse SEO d√©taill√©e...",
    "finish_reason": "stop"
  },
  "usage": {
    "prompt_tokens": 125,
    "completion_tokens": 275,
    "total_tokens": 400,
    "cost_usd": "0.008000"            // üÜï Co√ªt O3 plus √©lev√©
  },
  "model": "o3",
  "generation": "new",                // üÜï Indicateur g√©n√©ration
  "reasoning_effort": "high",         // üÜï Pour O3
  "execution_time_ms": 5200           // üÜï O3 plus lent mais meilleur
}
```

**R√©ponse Asynchrone** (jobs complexes) :
```json
{
  "job_id": "ai_job_def456",
  "status": "async",
  "task_id": "celery-task-789",
  "message": "Job started asynchronously with o3",
  "estimated_completion": "2024-12-20T15:05:00Z"
}
```

#### `GET /ai/openai/chat/`
**Mod√®les disponibles avec sp√©cificit√©s**

**R√©ponse enrichie** :
```json
{
  "models": [
    {
      "id": "o3",
      "name": "O3",
      "description": "Mod√®le de raisonnement avanc√©",
      "generation": "new",
      "supports_reasoning": true,
      "supports_temperature": false,
      "input_cost_per_token": 0.000020,
      "output_cost_per_token": 0.000020,
      "context_window": 200000,
      "recommended_for": ["analysis", "reasoning", "complex_tasks"],
      "default_params": {
        "reasoning_effort": "medium",
        "max_completion_tokens": 1000
      }
    },
    {
      "id": "o3-mini",
      "name": "O3 Mini", 
      "description": "O3 optimis√© co√ªt/performance",
      "generation": "new",
      "supports_reasoning": true,
      "supports_temperature": false,
      "input_cost_per_token": 0.000005,
      "output_cost_per_token": 0.000005,
      "context_window": 200000,
      "recommended_for": ["simple_reasoning", "cost_optimization"]
    },
    {
      "id": "gpt-4.1",
      "name": "GPT-4.1",
      "description": "GPT-4 enhanced",
      "generation": "new", 
      "supports_reasoning": false,
      "supports_temperature": true,
      "input_cost_per_token": 0.000004,
      "output_cost_per_token": 0.000004,
      "context_window": 200000,
      "recommended_for": ["creative", "large_context"],
      "default_params": {
        "temperature": 1.0,
        "max_completion_tokens": 10000
      }
    },
    {
      "id": "gpt-4o",
      "name": "GPT-4 Omni",
      "description": "Mod√®le multimodal √©quilibr√©",
      "generation": "legacy",
      "supports_reasoning": false,
      "supports_temperature": true,
      "input_cost_per_token": 0.000002,
      "output_cost_per_token": 0.000002,
      "context_window": 128000,
      "recommended_for": ["general", "multimodal", "cost_effective"],
      "default_params": {
        "temperature": 0.7,
        "max_tokens": 1000
      }
    }
  ],
  "generation_info": {
    "new": {
      "models": ["o3", "o3-mini", "gpt-4.1"],
      "features": ["structured_messages", "max_completion_tokens"],
      "limitations": ["no_temperature_for_o3"]
    },
    "legacy": {
      "models": ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"],
      "features": ["temperature", "max_tokens", "proven_stability"]
    }
  }
}
```

---

## üîÑ Services Internes Actualis√©s

### ChatService (Enhanced)

#### Auto-Configuration par Mod√®le
```python
@staticmethod
def create_chat_job(
    messages: List[Dict],
    brand, created_by,
    model: str = 'gpt-4o',
    temperature: Optional[float] = None,
    reasoning_effort: Optional[str] = None,
    max_completion_tokens: Optional[int] = None,
    **kwargs
) -> Dict[str, Any]:
    """Auto-configure selon le mod√®le"""
    
    # üéØ D√âTECTION G√âN√âRATION
    is_new_gen = model.startswith('o3') or model in ['gpt-4.1']
    
    # üéØ CONFIGURATION AUTOMATIQUE
    if is_new_gen:
        if model.startswith('o3'):
            # Configuration O3
            config = {
                'reasoning_effort': reasoning_effort or 'medium',
                'temperature': None,  # ‚ùå Non support√©
                'max_completion_tokens': max_completion_tokens or 1000
            }
        else:
            # Configuration GPT-4.1
            config = {
                'reasoning_effort': None,
                'temperature': temperature or 1.0,
                'max_completion_tokens': max_completion_tokens or 10000
            }
    else:
        # Configuration Legacy
        config = {
            'reasoning_effort': None,
            'temperature': temperature or 0.7,
            'max_tokens': max_completion_tokens or 1000
        }
```

### OpenAIService (Enhanced)

#### Conversion Automatique Messages
```python
def _convert_messages_to_structured(self, messages: List[Dict]) -> List[Dict]:
    """Convertit pour nouveaux mod√®les"""
    structured_messages = []
    
    for msg in messages:
        role = msg['role']
        content = msg['content']
        
        # üéØ MAPPING ROLES O3
        if role == 'system':
            role = 'developer'  # O3 utilise 'developer' au lieu de 'system'
        
        # üéØ STRUCTURE CONTENT
        if isinstance(content, str):
            structured_content = [{"type": "text", "text": content}]
        else:
            structured_content = content
        
        structured_messages.append({
            'role': role,
            'content': structured_content
        })
    
    return structured_messages
```

#### Payload Adaptatif
```python
def _build_payload(self, messages: List[Dict], model: str, **kwargs) -> Dict[str, Any]:
    """Construit payload selon le mod√®le"""
    
    is_new_gen = self._is_new_generation_model(model)
    
    # Messages format√©s selon g√©n√©ration
    if is_new_gen:
        formatted_messages = self._convert_messages_to_structured(messages)
    else:
        formatted_messages = messages
    
    payload = {"model": model, "messages": formatted_messages}
    
    # üéØ PARAM√àTRES SELON G√âN√âRATION
    if is_new_gen:
        if model.startswith('o3'):
            # Param√®tres O3 sp√©cifiques
            payload["reasoning_effort"] = kwargs.get('reasoning_effort', 'medium')
            if kwargs.get('max_completion_tokens'):
                payload["max_completion_tokens"] = kwargs['max_completion_tokens']
        else:
            # GPT-4.1 param√®tres
            payload.update({
                "temperature": kwargs.get('temperature', 1.0),
                "max_completion_tokens": kwargs.get('max_completion_tokens', 10000)
            })
    else:
        # Legacy param√®tres
        payload.update({
            "temperature": kwargs.get('temperature', 0.7),
            "max_tokens": kwargs.get('max_tokens', 1000)
        })
    
    return payload
```

---

## üí∞ Co√ªts par Mod√®le (Actualis√© 2024)

### Pricing O3 & GPT-4.1
```python
OPENAI_PRICING_2024 = {
    # üÜï NOUVEAUX MOD√àLES
    "o3": {
        "input": 0.000020,    # $20 / 1M tokens (estimation √©lev√©e)
        "output": 0.000020,   # $20 / 1M tokens 
        "reasoning_cost": True,  # Co√ªt du raisonnement inclus
        "performance": "highest"
    },
    "o3-mini": {
        "input": 0.000005,    # $5 / 1M tokens
        "output": 0.000005,   # $5 / 1M tokens
        "reasoning_cost": True,
        "performance": "high_efficiency"
    },
    "gpt-4.1": {
        "input": 0.000004,    # $4 / 1M tokens (estimation)
        "output": 0.000004,   # $4 / 1M tokens
        "performance": "enhanced"
    },
    
    # MOD√àLES EXISTANTS
    "gpt-4o": {
        "input": 0.000002,    # $2 / 1M tokens
        "output": 0.000002,   # $2 / 1M tokens
        "performance": "balanced"
    },
    "gpt-4-turbo": {
        "input": 0.000003,    # $3 / 1M tokens
        "output": 0.000003,   # $3 / 1M tokens
        "performance": "fast"
    }
}
```

### Calcul Automatique des Co√ªts
```python
def _get_cost_per_token(model: str) -> float:
    """Calcul pr√©cis selon mod√®le"""
    costs = OPENAI_PRICING_2024.get(model, {})
    
    # Moyenne input/output pour simplicit√©
    input_cost = costs.get('input', 0.000002)
    output_cost = costs.get('output', 0.000002)
    
    return (input_cost + output_cost) / 2
```

---

## üö® Validation et Gestion d'Erreurs

### Validation Sp√©cialis√©e par Mod√®le
```python
def validate(self, data):
    """Validation selon le mod√®le"""
    model = data.get('model', 'gpt-4o')
    
    # üéØ VALIDATION O3
    if model.startswith('o3'):
        if 'temperature' in data:
            raise ValidationError("O3 models don't support temperature parameter")
        
        if not data.get('reasoning_effort'):
            data['reasoning_effort'] = 'medium'
        
        # Conversion max_tokens ‚Üí max_completion_tokens
        if 'max_tokens' in data and 'max_completion_tokens' not in data:
            data['max_completion_tokens'] = data.pop('max_tokens')
    
    # üéØ VALIDATION GPT-4.1
    elif model == 'gpt-4.1':
        if not data.get('temperature'):
            data['temperature'] = 1.0
        
        # Conversion vers max_completion_tokens
        if 'max_tokens' in data:
            data['max_completion_tokens'] = data.pop('max_tokens')
    
    # üéØ VALIDATION LEGACY
    else:
        if 'reasoning_effort' in data:
            raise ValidationError(f"Model {model} doesn't support reasoning_effort")
        
        if not data.get('temperature'):
            data['temperature'] = 0.7
    
    return data
```

### Erreurs Sp√©cifiques O3
```json
// Temperature sur O3
{
  "error": "O3 models don't support temperature parameter",
  "error_code": "INVALID_PARAMETER_FOR_MODEL",
  "model": "o3",
  "invalid_params": ["temperature"],
  "suggested_params": ["reasoning_effort", "max_completion_tokens"]
}

// Reasoning effort sur legacy
{
  "error": "Model gpt-4o doesn't support reasoning_effort parameter", 
  "error_code": "INVALID_PARAMETER_FOR_MODEL",
  "model": "gpt-4o",
  "invalid_params": ["reasoning_effort"],
  "suggested_params": ["temperature", "max_tokens"]
}
```

---

## üìà Performance et Monitoring

### M√©triques par G√©n√©ration
```python
# Tracking automatique am√©lior√©
{
  "model": "o3",
  "generation": "new",
  "execution_time_ms": 5200,        # O3 plus lent mais meilleur
  "reasoning_effort": "high",       # Impact sur temps/co√ªt
  "cost_usd": "0.008000",          # Co√ªt √©lev√© O3
  "quality_score": 9.8,            # Qualit√© sup√©rieure
  "reasoning_steps": 47            # üÜï √âtapes de raisonnement
}
```

### Recommandations Automatiques
```python
def get_model_recommendation(task_complexity: float, budget_constraint: float):
    """Recommandation intelligente de mod√®le"""
    
    if task_complexity > 0.9 and budget_constraint > 0.7:
        return "o3"  # T√¢ches complexes, budget confortable
    elif task_complexity > 0.7 and budget_constraint > 0.5:
        return "o3-mini"  # Bon compromis qualit√©/prix
    elif task_complexity > 0.5:
        return "gpt-4.1"  # Cr√©ativit√© et contexte large
    else:
        return "gpt-4o"  # T√¢ches standard, √©conomique
```

---

## üîÑ Migration et Compatibilit√©

### Migration Automatique des Param√®tres
```python
# Conversion automatique legacy ‚Üí new generation
if model.startswith('o3') and 'max_tokens' in params:
    params['max_completion_tokens'] = params.pop('max_tokens')
    del params['temperature']  # Retirer temperature pour O3

if model == 'gpt-4.1' and 'max_tokens' in params:
    params['max_completion_tokens'] = params.pop('max_tokens')
```

### R√©trocompatibilit√©
- **API legacy** : Continue de fonctionner avec auto-conversion
- **Param√®tres legacy** : Convertis automatiquement selon le mod√®le
- **R√©ponses** : Format uniforme avec champs additionnels pour nouveaux mod√®les

---

## üí° Bonnes Pratiques Mises √† Jour

### Choix de Mod√®le Optimal
1. **O3** : Analyse complexe, raisonnement critique, recherche
2. **O3-mini** : Bon compromis pour t√¢ches de raisonnement courantes
3. **GPT-4.1** : Cr√©ation de contenu, contexte large, cr√©ativit√©
4. **GPT-4o** : T√¢ches standard, multimodal, efficacit√©

### Optimisation des Co√ªts O3
1. **reasoning_effort = 'low'** pour t√¢ches simples
2. **Batch processing** pour r√©duire les co√ªts fixes
3. **Cache intelligent** pour requ√™tes similaires
4. **Fallback** : O3-mini puis GPT-4o selon budget

### Monitoring Avanc√©
1. **Tracking par g√©n√©ration** pour analyser ROI
2. **Alertes co√ªt** sp√©cifiques O3 (plus cher)
3. **M√©triques qualit√©** pour justifier surco√ªt O3
4. **Usage patterns** pour optimisation future

---

**Cette documentation couvre l'int√©gration OpenAI compl√®te avec support O3/GPT-4.1. L'architecture auto-configure intelligemment selon le mod√®le choisi, garantissant performances optimales et compatibilit√© totale.**